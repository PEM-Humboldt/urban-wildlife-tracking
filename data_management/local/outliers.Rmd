---
title: "In depth analysis of errors in the movement data and deletion of outliers"
author: "Marius Bottin"
date: "`r Sys.Date()`"
output: 
    github_document:
      toc: true
      toc_depth: 3
      dev: 'jpeg'
always_allow_html: true
---


```{r setup, echo=F,message=F,warning=FALSE,results='hide'}
stopifnot(
require(RPostgreSQL)
&
require(rpostgis)
&
require(sp)
&
require(geodata)
&
require(DBI)
&
require(ctmm)
&
require(move)
&
require(rgeos)
&
require(lubridate)
&
require(leaflet)
&
require(kableExtra)
)

knitr::opts_chunk$set(cache=T,tidy.opts = list(width.cutoff = 70), tidy = TRUE, fig.path="Fig/outliers_",echo=T)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "footnotesize","\n\n", x, "\n\n \\normalsize\n\n")
})
options(knitr.kable.NA = "---")
dirData <- "../../../uwt_repo_data/extra_spatial/"
set.seed(9835)
extraSp<-dbConnect(drv = PostgreSQL(), dbname="move_extra_sp")
# For avoiding setting the connection name in the SQL chunks:
knitr::opts_chunk$set(connection=extraSp)
```

# Getting the general data from movebank

If needed, more details are presented in the document [importingCleaningFormatting.Rmd](./importingCleaningFormatting.md)

```{r}
passWord <- read.csv("password.csv",h=F)[1,1]
lgin <- movebankLogin(username="Humboldt_AreaMetropolitana",password=passWord)
study_id<- getMovebankID("Rastreo fauna área metropolitana del Valle de Aburrá, Colombia",login=lgin)
```



# Importing data, by individual

Before doing that, make sure that you ran the code contained in file [SpatialErrorCalibration.Rmd](SpatialErrorCalibration.md).
Various processes needed to be done, in particular the relationship between names of the animals in the movebank data and the calibration local datasets.

***********************

**Note: timestamps and timezones**

The data from movevank is usually given with Universal Time Zone (UTZ) timestamp.
For it to make more ecological sense, we need to put the timestamp in the local timezone ("America/Bogota").
There might be some options to download it directly in local timezone (TODO) but here we will transform the timestamp with the package `lubridate`

***********************


```{r message=F, warning=F}
load("./tabNames.RData")
rawList <- list()
for(i in 1:nrow(tabNames))
{
  rawList[[tabNames$aniNames[i]]]<-list()
  rawList[[i]]$moveData=getMovebankData(study = study_id, login = lgin,animalName = tabNames$aniNames[i])
  timestamps(rawList[[i]]$moveData) <- with_tz(timestamps(rawList[[i]]$moveData),tz="America/Bogota")
  rawList[[i]]$ctmmData=as.telemetry(rawList[[i]]$moveData)
  rawList[[i]]$ctmmData$class<-NULL
}
```

# Very basic filter

Let's first remove all the relocation which are not in the Antioquia department!
For that, you will need the local database with geographic extra information as explained in [localExtraSpatialDataDB.Rmd](./localExtraSpatialDataDB.md) (only the municipality data is mandatory)

We need to be sure that `move`-formatted data and `ctmm`-formatted data are exactly the same, and that 

```{r}
stopifnot(sapply(rawList,function(x)
{
  m1<-match(x$moveData$timestamp,x$ctmmData$timestamp)
  all(m1==(1:length(m1)))
}))
```

```{r}
Colombia<-pgGetGeom(extraSp,query = "SELECT ST_Union(the_geom) geom FROM mpio ")
inColombia <- lapply(rawList,function(rl,poly)
  !is.na(over(rl$moveData,poly))
  ,poly=Colombia)
```

The number of points out of Colombia is:
```{r}
sapply(sapply(inColombia,`!`),sum)
```


```{r}
for(i in names(inColombia))
{
  rawList[[i]]$moveData<-rawList[[i]]$moveData[inColombia[[i]]]
  rawList[[i]]$ctmmData<-subset(rawList[[i]]$ctmmData,inColombia[[i]])
}
```

# Analyses of spatial errors

In the document [importingCleaningFormatting.Rmd](./importingCleaningFormatting.md), you may see how to use the function from `ctmm` and `move` for managing movement data.
However, there are some decisions to take, animal by animal, in order to delete erroneous data from the individual datasets.

The criteria for pointing outliers may be extracted from:

* HDOP data associated with each relocation
* distance from the previous and the following point
* average speed calculated on every trajectory between 2 relocation points
* it is important to take into account the fact that some of the relocation might be missing and therefore the comparison of distances and speeds may be irrelevant
* `ctmm` uses  calibration datasets to fit UERE models (see [SpatialErrorCalibration.Rmd](SpatialErrorCalibration.md))
* `ctmm` also allows to create semi-variogram that apparently may be of use for searching for outliers

With all these criteria, it is possible to point out which are the points which may be erroneous.
However, it seems that there are no real procedure to automatically suppress outliers based on them.
Each outlier should be evaluated visually, because it might be a real change in the movement behavior of the animals.


**Note that the goal here is to suppress the data that seems erroneous, in a case where we want to fit a movement model (for example in a case of homerange estimation) we might want to suppress the points that are adding noise in the autocorrelation structure of the dataset... THAT IS NOT THE CASE HERE**

## Before starting filtering


```{r}
stopifnot(sapply(rawList,function(x)
{
  m1<-match(x$moveData$timestamp,x$ctmmData$timestamp)
  all(m1==(1:length(m1)))
}))
```





## Loading the User Equivalent Range Error (UERE)

The UERE have been fitted in [SpatialErrorCalibration.Rmd](./SpatialErrorCalibration.md).


```{r}
load("uere.RData")
```

The values applied to the UERE, in meters, are the following:

```{r}
(uere_val<-sapply(UERE_list,function(x)x$UERE[1]))
mean(uere_val)
```


Note that these values are much higher than the usual values applied without calibration to the devices (10m.)


Let's apply the values to the 

```{r}
for(i in 1:length(rawList)){
  if(names(rawList)[i] %in% names(UERE_list))
  {
    uere(rawList[[i]]$ctmmData)<-NULL
    uere(rawList[[i]]$ctmmData)<-UERE_list[[names(rawList)[i]]]}
}
```



## Checking Error criteria

Let's first create a list with only the calibrated data:

```{r}
namesCalib <- sort(names(UERE_list))
rawListCalib <- rawList[namesCalib] 
```

In order to have an idea about the criteria to apply for filtering erroneous data, let's first have a look at the highest quantiles of the HDOP measurements:

```{r}
tot_hdop<-unlist(lapply(rawList,function(x)x$ctmmData$HDOP))
quantile(tot_hdop,c(.9,.95,.975,.99,.999))
```
We can reasonably suppress all values superior to 2.5, which also mean, knowing the average UERE value for the devices, that we suppress approximately all the points where the estimated error is around 75m.



```{r}
tot_varxy<-unlist(lapply(rawListCalib,function(x)x$ctmmData$VAR.xy))
quantile(tot_varxy,c(.9,.95,.975,.99,.999))
```
We can reasonably suppress all values superior to 5000 m2

## Calculating Error criteria

Other criteria:

```{r warning=F}
err_crit_calc<-function(rLid){
  spPt<-spTransform(SpatialPointsDataFrame.telemetry(rLid$ctmmData),"+proj=tmerc +lat_0=4.596200416666666 +lon_0=-74.07750791666666 +k=1 +x_0=1000000 +y_0=1000000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs")
  distances<-numeric(length(spPt)-1)
  for(i in 2:length(spPt))
    {
      distances[i-1]<-gDistance(spPt[i,],spPt[i-1,])
    }
  tl<-as.numeric(diff(rLid$ctmmData$timestamp))
  res <- data.frame(timestamp=rLid$ctmmData$timestamp,
             dist_from=c(NA,distances),
             dist_to=c(distances,NA),
             tl_from=c(NA,tl),
             tl_to=c(tl,NA),
             speed_from_m_s=c(NA,distances/tl),
             speed_to_m_s=c(distances/tl,NA),
             hdop=rLid$ctmmData$HDOP,
             VAR.xy=ifelse("VAR.xy"%in%colnames(rLid$ctmmData),rLid$ctmmData$VAR.xy,NA),
             VAR.v=ifelse("VAR.v"%in%colnames(rLid$ctmmData),rLid$ctmmData$VAR.v,NA)
             )
  res$sum_dist_from_to <- res$dist_from + res$dist_to
  res$sum_tl_from_to <- res$tl_from + res$tl_to
  res$aver_speed_from_to <- res$sum_dist_from_to/res$sum_tl_from_to
  return(res)
}
errCrit<-lapply(rawList,err_crit_calc)

```

What we obtain for each individual is a table such as this (example Garza1, first 20 rows):

```{r}
errCrit$Garza1 %>% head(n=20) %>% kable()
```

## Applying the filters


Applying the HDOP threshold (uncalibrated data) or the VAR.xy threshold (calibrated data):

```{r}
critVARxy<-5000
critHdop<-2.5
toSupp<-list()
for(i in 1:length(rawList)){
  calibrated <- names(rawList)[i]%in%namesCalib
  if(calibrated){
    toSupp[[names(rawList)[i]]]<-data.frame(
      timestamp=errCrit[[names(rawList)[i]]]$timestamp,
      supp=errCrit[[names(rawList)[i]]]$VAR.xy>critVARxy
    )
  }else{
    toSupp[[names(rawList)[i]]]<-data.frame(
      timestamp=errCrit[[names(rawList)[i]]]$timestamp,
      supp=errCrit[[names(rawList)[i]]]$hdop>critHdop
    )
    
  }
}
```

```{r}
stopifnot(mapply(function(x,y){
  m<-match(x$moveData$timestamp,y$timestamp)
  all(m==1:length(m))
},rawList,toSupp))
```

Number of relocation to delete with these criteria

```{r}
sapply(toSupp,function(x)sum(x$supp))
```

Percentage of data to delete with these criteria:
```{r}
sapply(toSupp,function(x)sum(x$supp)/nrow(x))*100
```

The percentage is usually very low, but result in deleting all the data in Zarigueya2, and a significant percentage of Phimosus01 and PerezosoZorro1.

Additional to the criteria about global HDOP and VAR.xy values.

We will suppress the points when all these conditions are met:

* The speed is among the 5% highest
* The speed is superior to 15 m/s (54 km/h) for birds and 5 m/s (15-20 km/h) for others
* The HDOP is among the 5% highest

```{r}
propSpeed<-.95
propHdop<-.95
speedBird<-15
speedOthers<-5
for(i in 1:length(errCrit))
{
  A1<-(rank(errCrit[[i]]$aver_speed_from_to)-1)/nrow(errCrit[[i]])>propSpeed
    if(
      grepl("Guacharaca",names(errCrit)[i])|
      grepl("Phimosus",names(errCrit)[i])|
      grepl("Pigua",names(errCrit)[i])|
      grepl("Garza",names(errCrit)[i])|
      grepl("Asio",names(errCrit)[i])
      )
    {B<-errCrit[[i]]$aver_speed_from_to>speedBird}else{B<-errCrit[[i]]$aver_speed_from_to>speedOthers}
  A2<-(rank(errCrit[[i]]$hdop)-1)/nrow(errCrit[[i]])>propHdop
  toSupp[[i]]$supp<-toSupp[[i]]$supp|(!is.na(errCrit[[i]]$aver_speed_from_to)&A1&B&A2)
}
```



Number of relocation to delete with these criteria

```{r}
sapply(toSupp,function(x)sum(x$supp))
```

Percentage of data to delete with these criteria:
```{r}
sapply(toSupp,function(x)sum(x$supp)/nrow(x))*100
```


Now we apply the filters on the raw data from movebank to create a filtered dataset:

```{r}
stopifnot(mapply(function(x,y){
  m<-match(x$moveData$timestamp,y$timestamp)
  all(m==1:length(m))
},rawList,toSupp))
```


```{r}
filteredList<-list()
for(i in names(toSupp))
{
  filteredList[[i]]$moveData<-rawList[[i]]$moveData[!toSupp[[i]]$supp]
  filteredList[[i]]$ctmmData<-subset(rawList[[i]]$ctmmData,!toSupp[[i]]$supp)
}
filteredList<-filteredList[sapply(filteredList,function(x)nrow(x$ctmmData)>0)]

```

## Saving the files for later use

```{r}
save(filteredList,file="filteredList.RData")
save(errCrit,file="spatErrCrit.RData")
```